# MyBTP-2021
My BTP - 2021

For scraping:

The get_proxy_code.py file generates proxies by dynamically scraping from the internet and discards the ones not usable (which are already used by other people on the internet and are thus 
blocked). It generates a .txt file of http proxy links which are used in gs_main.py

We get list of titles of paper for a given author for papers written after 2015 (can be changed) with gs_main.py file. It is then used in selenium_ss.py file to scrape abstracts and 
calls the get_arxiv.py file when an exception of the paper not existing is thrown.


Modelling:

To run the model, execute main.py. This files calls helper.py and autoencoder.py. main.py and autoencoder.py helps run the ctm model. 

helper.py contains helper functions that generate wordclouds, UMAPs and use matplotlib to visualize them.

NN_word2Vec.ipynb creates the neural network model on top of the CTM model. It uses the topics generated by main.py. To obtain the retrofitted vectors, run the input file being used
in this notebook (obtained via the trained model) in this Github repo by Manaal Faruqui: https://github.com/mfaruqui/retrofitting
It will generate a .txt file with updated vector representations, that are used further to obtain results.

